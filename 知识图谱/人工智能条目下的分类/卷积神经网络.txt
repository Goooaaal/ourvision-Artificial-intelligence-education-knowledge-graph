卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。
卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。


== 概览 ==


== 发展 ==


== 结构 ==


=== 卷积层 ===
卷积层（Convolutional layer），卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法最佳化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网路能从低级特征中迭代提取更复杂的特征。


=== 线性整流层 ===
线性整流层（Rectified Linear Units layer, ReLU layer）使用线性整流（Rectified Linear Units, ReLU）
  
    
      
        f
        (
        x
        )
        =
        max
        (
        0
        ,
        x
        )
      
    
    {\displaystyle f(x)=\max(0,x)}
  作为这一层神经的激励函数（Activation function）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。
事实上，其他的一些函数也可以用于增强网络的非线性特性，如双曲正切函数 
  
    
      
        f
        (
        x
        )
        =
        tanh
        ⁡
        (
        x
        )
      
    
    {\displaystyle f(x)=\tanh(x)}
  , 
  
    
      
        f
        (
        x
        )
        =
        
          |
        
        tanh
        ⁡
        (
        x
        )
        
          |
        
      
    
    {\displaystyle f(x)=|\tanh(x)|}
  ，或者Sigmoid函数
  
    
      
        f
        (
        x
        )
        =
        (
        1
        +
        
          e
          
            −
            x
          
        
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle f(x)=(1+e^{-x})^{-1}}
  。相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍，而并不会对模型的泛化准确度造成显着影响。


=== 池化层(Pooling Layer) ===

池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。
池化层通常会分别作用于每个输入的特征并减小其大小。目前最常用形式的池化层是每隔2个元素从图像划分出
  
    
      
        2
        ×
        2
      
    
    {\displaystyle 2\times 2}
  的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。
除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“L2-范数池化”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。
由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，甚至不再使用池化层。


=== 损失函数层 ===
损失函数层（loss layer）用于决定训练过程如何来“惩罚”网络的预测结果和真实结果之间的差异，它通常是网络的最后一层。各种不同的损失函数适用于不同类型的任务。例如，Softmax交叉熵损失函数常常被用于在K个类别中选出一个，而Sigmoid交叉熵损失函数常常用于多个独立的二分类问题。欧几里德损失函数常常用于结果取值范围为任意实数的问题。


== 应用 ==


=== 影像辨识 ===
卷积神经网络通常在影像辨识别系统中使用。


=== 视讯分析 ===
相比影像辨识问题，视讯分析要难许多。CNN也常被用于这类问题。


=== 自然语言处理 ===
卷积神经网络也常被用于自然语言处理。 CNN的模型被证明可以有效的处理各种自然语言处理的问题，如语义分析、搜索结果提取、句子建模 、分类、预测、和其他传统的NLP任务 等。


=== 药物发现 ===
卷积神经网路已在药物发现中使用。卷积神经网络被用来预测的分子与蛋白质之间的相互作用，以此来寻找靶向位点，寻找出更可能安全和有效的潜在治疗方法。


=== 围棋 ===

卷积神经网路在计算机围棋领域也被使用。2016年3月，AlphaGo对战李世乭的比赛，展示了深度学习在围棋领域的重大突破。


== 微调（fine-tuning） ==


== 可用包 ==
roNNie: 是一个简易入门级框架,使用Tensorflow 计算层.可於python下载 pip3 ronnie
Caffe: Caffe包含了CNN使用最广泛的库。它由伯克利视觉和学习中心（BVLC）研发，拥有比一般实现更好的结构和更快的速度。同时支持CPU和GPU计算，底层由C++实现，并封装了Python和MATLAB的接口。
Torch7（www.torch.ch）
OverFeat
Cuda-convnet
MatConvnet
Theano：用Python实现的神经网络包
TensorFlow
Paddlepaddle(www.paddlepaddle.org)


== 参考 ==
