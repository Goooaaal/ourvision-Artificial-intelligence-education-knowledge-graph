递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），另一种是结构递归神经网络（recursive neural network）。时间递归神经网络的神经元间连接构成矩阵，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。RNN一般指代时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。
时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果。


== 时间递归神经网络（Recurrent） ==


=== 编码器 ===
递归神经网络将输入序列
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
  编码为一个固定长度的隐藏状态
  
    
      
        
          
            
              h
              →
            
          
        
      
    
    {\displaystyle {\vec {h}}}
  ，这里有（用自然语言处理作为例子）：

  
    
      
        
          
            
              x
              →
            
          
        
        =
        (
        
          x
          
            t
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            1
          
        
        )
      
    
    {\displaystyle {\vec {x}}=(x_{t},...,x_{1})}
   是输入序列，比如编码为数字的一系列词语，整个序列就是完整的句子。

  
    
      
        
          
            
              
                h
                
                  t
                
              
              →
            
          
        
        =
        f
        (
        
          x
          
            t
          
        
        ,
        
          
            
              
                h
                
                  t
                  −
                  1
                
              
              →
            
          
        
        )
      
    
    {\displaystyle {\vec {h_{t}}}=f(x_{t},{\vec {h_{t-1}}})}
  是随时间更新的隐藏状态。当新的词语输入到方程中，之前的状态
  
    
      
        
          
            
              
                h
                
                  t
                  −
                  1
                
              
              →
            
          
        
      
    
    {\displaystyle {\vec {h_{t-1}}}}
  就转换为和当前输入
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  相关的
  
    
      
        
          
            
              
                h
                
                  t
                
              
              →
            
          
        
      
    
    {\displaystyle {\vec {h_{t}}}}
  ，距离当前时间越长，越早输入的序列，在更新后的状态中所占权重越小，从而表现出时间相关性。
其中，计算隐藏状态的方程
  
    
      
        f
        (
        x
        ,
        h
        )
      
    
    {\displaystyle f(x,h)}
  是一个非线性方程，可以是简单的Logistic方程（tanh），也可以是复杂的LSTM单元（Long-Short Term Memory）。  而有了隐藏状态序列，就可以对下一个出现的词语进行预测：

  
    
      
        p
        (
        
          y
          
            t
          
        
        )
        =
        p
        (
        
          y
          
            t
          
        
        
        
          |
        
        
        
          y
          
            t
            −
            1
          
        
        ,
        .
        .
        .
        ,
        
          y
          
            1
          
        
        )
      
    
    {\displaystyle p(y_{t})=p(y_{t}\,|\,y_{t-1},...,y_{1})}
  ，其中
  
    
      
        
          y
          
            t
          
        
      
    
    {\displaystyle y_{t}}
  是第t个位置上的输出，它的概率基于之前输出的所有词语。
以上概率可以通过隐藏状态来计算：
  
    
      
        p
        (
        
          y
          
            t
          
        
        )
        =
        g
        (
        
          y
          
            t
            −
            1
          
        
        ,
        
          
            
              
                h
                
                  t
                
              
              →
            
          
        
        ,
        
          
            
              c
              →
            
          
        
        )
      
    
    {\displaystyle p(y_{t})=g(y_{t-1},{\vec {h_{t}}},{\vec {c}})}
  ，
  
    
      
        
          
            
              c
              →
            
          
        
      
    
    {\displaystyle {\vec {c}}}
  是所有隐藏状态的编码，总含了所有隐藏状态，比如可以是简单的最终隐藏状态
  
    
      
        
          
            
              
                h
                
                  t
                
              
              →
            
          
        
      
    
    {\displaystyle {\vec {h_{t}}}}
  ，也可以是非线性方程的输出
  
    
      
        f
        (
        
          h
          
            t
          
        
        ,
        .
        .
        .
        ,
        
          h
          
            1
          
        
        )
      
    
    {\displaystyle f(h_{t},...,h_{1})}
  。因为隐藏状态t就编码了第t个输入前全部的输入信息，
  
    
      
        
          y
          
            t
          
        
      
    
    {\displaystyle y_{t}}
  也迭代式地隐含了之前的全部输出信息，所以这个概率计算方法是合理的。
这里的非线性方程
  
    
      
        g
        (
        y
        ,
        h
        ,
        c
        )
      
    
    {\displaystyle g(y,h,c)}
  可以是一个复杂的前馈神经网络，也可以是简单的非线性方程（但有可能因此无法适应复杂的条件而得不到任何有用结果）。给出的概率可以用监督学习的方法优化内部参数来给出翻译，也可以训练后用来给可能的备选词语，用计算其第j个备选词
  
    
      
        
          y
          
            t
            ,
            j
          
        
      
    
    {\displaystyle y_{t,j}}
  出现在下一位置的概率，给它们排序。排序后用于其它翻译系统，可以提升翻译质量。


=== 解码器 ===
更复杂的情况下复发神经网络还可以结合编码器作为解码器（Decoder），用于将编码后（Encoded）的信息解码为人类可识别的信息。也就是上述例子中的
  
    
      
        
          y
          
            t
          
        
        =
        f
        (
        
          y
          
            t
            −
            1
          
        
        ,
        
          h
          
            t
          
        
        ,
        c
        )
      
    
    {\displaystyle y_{t}=f(y_{t-1},h_{t},c)}
  过程，当中非线性模型
  
    
      
        f
      
    
    {\displaystyle f}
  就是作为输出的复发神经网络。只是在解码过程中，隐藏状态因为是解码器的参数，所以为了发挥时间序列的特性，需要对
  
    
      
        
          h
          
            t
          
          ′
        
      
    
    {\displaystyle h_{t}'}
  继续进行迭代：

  
    
      
        
          h
          
            t
          
          ′
        
        =
        g
        (
        
          h
          
            t
            −
            1
          
        
        ,
        
          y
          
            t
            −
            1
          
        
        ,
        c
        )
      
    
    {\displaystyle h_{t}'=g(h_{t-1},y_{t-1},c)}
  ，
  
    
      
        
          
            
              c
              →
            
          
        
      
    
    {\displaystyle {\vec {c}}}
  是解码器传递给编码器的参数，是解码器中状态的summary。
  
    
      
        
          h
          
            t
          
          ′
        
      
    
    {\displaystyle h_{t}'}
  是解码器的隐藏状态。
  
    
      
        
          y
          
            t
          
        
      
    
    {\displaystyle y_{t}}
  是第t个输出。
当输入仍为
  
    
      
        
          
            
              x
              →
            
          
        
        =
        (
        
          x
          
            t
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            1
          
        
        )
      
    
    {\displaystyle {\vec {x}}=(x_{t},...,x_{1})}
  ，输出是
  
    
      
        
          
            
              y
              →
            
          
        
        =
        (
        
          y
          
            t
          
        
        ,
        .
        .
        .
        ,
        
          y
          
            1
          
        
        )
      
    
    {\displaystyle {\vec {y}}=(y_{t},...,y_{1})}
  ，最大化条件概率
  
    
      
        P
        (
        
          
            
              y
              →
            
          
        
        
        
          |
        
        
        
          
            
              x
              →
            
          
        
        )
      
    
    {\displaystyle P({\vec {y}}\,|\,{\vec {x}})}
  后就是最好的翻译结果。


=== 双向读取 ===
用两个复发神经网络双向读取一个序列可以使人工智能获得“注意力”。简单的做法是将一个句子分别从两个方向编码为两个隐藏状态，然后将两个
  
    
      
        
          
            
              h
              →
            
          
        
      
    
    {\displaystyle {\vec {h}}}
  拼接在一起作为隐藏状态。 这种方法能提高模型表现的原因之一可能是因为不同方向的读取在输入和输出之间创造了更多短期依赖关系，从而被RNN中的LSTM单元（及其变体）捕捉，例如在实验中发现颠倒输入序列的顺序（但不改变输出的顺序）可以意外达到提高表现的效果。


== 结构递归神经网络（Recursive） ==
结构递归神经网络是一类用结构递归的方式构建的网络，比如说递归自编码机（Recursive Autoencoder），在自然语言处理的神经网络分析方法中用于解析语句。 


== 参考 ==


== 外部连结 ==
A Critical Review of Recurrent Neural Networks for Sequence Learning
