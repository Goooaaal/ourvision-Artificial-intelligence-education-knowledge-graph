长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间递归神经网络（RNN），论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。
LSTM的表现通常比时间递归神经网络及隐马尔科夫模型（HMM）更好，比如用在不分段连续手写识别上。2009年，用LSTM构建的人工神经网络模型赢得过ICDAR手写识别比赛冠军。LSTM还普遍用于自主语音识别，2013年运用TIMIT自然演讲资料库达成17.7%错误率的纪录。作为非线性模型，LSTM可作为复杂的非线性单元用于构造更大型深度神经网络。


== 结构 ==

LSTM是一种含有LSTM区块（blocks）或其他的一种类神经网路，文献或其他资料中LSTM区块可能被描述成智慧型网路单元，因为它可以记忆不定时间长度的数值，区块中有一个gate能够决定input是否重要到能被记住及能不能被输出output。
右图底下是四个S函数单元，最左边函数依情况可能成为区块的input，右边三个会经过gate决定input是否能传入区块，左边第二个为input gate，如果这里产出近似於零，将把这里的值挡住，不会进到下一层。左边第三个是forget gate，当这产生值近似於零，将把区块里记住的值忘掉。第四个也就是最右边的input为output gate，他可以决定在区块记忆中的input是否能输出 。
LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit），根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate。


== 训练方法 ==
为了最小化训练误差，梯度下降法（Gradient descent）如：应用时序性倒传递演算法，可用来依据错误修改每次的权重。梯度下降法在递回神经网路（RNN）中主要的问题初次在1991年发现，就是误差梯度随着事件间的时间长度成指数般的消失。当设置了LSTM 区块时，误差也随着倒回计算，从output影响回input阶段的每一个gate，直到这个数值被过滤掉。因此正常的倒传递类神经是一个有效训练LSTM区块记住长时间数值的方法。


== 应用 ==


== 参见 ==
人工神经网络
前额叶皮质基底节工作记忆（PBWM）
递归神经网络
时间序列


== 完整阅读 ==
理解LSTM网络，作者Christopher Olah，更新于2015年八月。


== 参考 ==
